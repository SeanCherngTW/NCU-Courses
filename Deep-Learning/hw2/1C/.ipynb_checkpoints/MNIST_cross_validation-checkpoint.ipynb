{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"../MNIST_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = mnist.train.images\n",
    "y_test = to_categorical(mnist.train.labels, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_folds(num_folds=3):\n",
    "    train_length = len(mnist.train.labels)\n",
    "    fold_size = int(train_length / num_folds)\n",
    "    X_train_folds = []\n",
    "    y_train_folds = []\n",
    "    for i in range(num_folds):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size if i + 1 < num_folds else train_length\n",
    "        images = mnist.train.images[start : end]\n",
    "        labels = mnist.train.labels[start : end]\n",
    "        labels = to_categorical(labels, num_classes=10)\n",
    "        X_train_folds.append(images)\n",
    "        y_train_folds.append(labels)\n",
    "\n",
    "    return X_train_folds, y_train_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_input():\n",
    "    with tf.name_scope('input'):\n",
    "        X = tf.placeholder('float', [None, 784], name='input_x')\n",
    "        y = tf.placeholder('float', [None, 10], name='label_y')\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_layer(input_dim, output_dim, inputs, name, activation_function=None):\n",
    "    name_W = 'W_' + name\n",
    "    name_b = 'b_' + name\n",
    "    \n",
    "    with tf.name_scope(name):\n",
    "        with tf.name_scope('weight'):\n",
    "            W = tf.get_variable(shape=[input_dim, output_dim], initializer=tf.contrib.keras.initializers.he_normal(), name=name_W)\n",
    "            tf.summary.histogram(name + '/weight', W)\n",
    "        with tf.name_scope('bias'):\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[1, output_dim]), name=name_b)\n",
    "            tf.summary.histogram(name + '/bias', b)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            Wx_plus_b = tf.add(tf.matmul(inputs, W), b)\n",
    "        if activation_function is None:\n",
    "            outputs = Wx_plus_b\n",
    "        else:\n",
    "            outputs = activation_function(Wx_plus_b)\n",
    "        tf.summary.histogram(name + '/output', outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(X, n, activation):\n",
    "    h1 = add_layer(input_dim=784, output_dim=n, inputs=X, name='hidden_layer_1', activation_function=activation)\n",
    "    h2 = add_layer(input_dim=n, output_dim=n, inputs=h1, name='hidden_layer_2', activation_function=activation)\n",
    "    h3 = add_layer(input_dim=n, output_dim=n, inputs=h2, name='hidden_layer_3', activation_function=activation)\n",
    "    h4 = add_layer(input_dim=n, output_dim=n, inputs=h3, name='hidden_layer_4', activation_function=activation)\n",
    "    h5 = add_layer(input_dim=n, output_dim=n, inputs=h4, name='hidden_layer_5', activation_function=activation)\n",
    "    y_hat = add_layer(input_dim=n, output_dim=10, inputs=h5, name='output_layer', activation_function=tf.nn.softmax)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_evaluation(y, y_hat, learning_rate):\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=y_hat))\n",
    "    tf.summary.scalar('cross_entropy', loss_function)\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_function)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correction_prediction'):\n",
    "            correct_prediction_count = tf.equal(tf.argmax(y, 1), tf.argmax(y_hat, 1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            acc = tf.reduce_mean(tf.cast(correct_prediction_count, 'float'))\n",
    "    tf.summary.scalar('accuracy', acc)\n",
    "    \n",
    "    return loss_function, optimizer, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN(epoch, n_neurons, learning_rate, activation, batch_size, early_stopping, restore_model_name=None):\n",
    "    X_train_folds, y_train_folds = split_folds()\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    training_id = '[n_neurons_%d, learning_rate_%f, activation_%s, batch_size_%d]' % (n_neurons, learning_rate, activation, batch_size)\n",
    "    training_id += \"-\" + datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    logdir = \"tf_logs/{}/\".format(training_id)\n",
    "    \n",
    "    X, y = init_input()\n",
    "    y_hat = build_network(X, n_neurons, activation)\n",
    "    loss_function, optimizer, acc = init_evaluation(y, y_hat, learning_rate)\n",
    "    \n",
    "    batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    start_epoch = 0\n",
    "    iteration = 0\n",
    "    \n",
    "    print('[n_neurons=%d, learning_rate=%f, activation=%s, batch_size=%d]' % (n_neurons, learning_rate, activation, batch_size))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        if restore_model_name:\n",
    "            saver.restore(sess, \"regular_train/\" + restore_model_name)\n",
    "            start_epoch = int(restore_model_name.split('.')[0].split('_')[-1])\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "        merged = tf.summary.merge_all()    \n",
    "        writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "        \n",
    "        for i in range(len(X_train_folds)):\n",
    "            best_vali_acc = 0.0\n",
    "            n = 0\n",
    "            # Always takes the first fold as validation fold, than move the fold to the bottom after each iteration\n",
    "            print('FOLD %d' % (i + 1))\n",
    "            vali_X = X_train_folds.pop(0)\n",
    "            vali_y = y_train_folds.pop(0)\n",
    "            vali_loss_list = []\n",
    "            vali_acc_list = []\n",
    "            for i in range(start_epoch, epoch):\n",
    "                for X_train_fold, y_train_fold in zip(X_train_folds, y_train_folds): \n",
    "                    for j in range(batch):\n",
    "                        batch_x = X_train_fold[j * batch_size : (j + 1) * batch_size]\n",
    "                        batch_y = y_train_fold[j * batch_size : (j + 1) * batch_size]\n",
    "                        sess.run(optimizer, feed_dict={X: batch_x, y: batch_y})\n",
    "\n",
    "                iteration += 1\n",
    "                result = sess.run(merged, feed_dict={X: vali_X, y: vali_y})\n",
    "                writer.add_summary(result, iteration)\n",
    "\n",
    "                vali_loss = sess.run(loss_function, feed_dict={X: vali_X, y: vali_y})\n",
    "                vali_acc = sess.run(acc, feed_dict={X: vali_X, y: vali_y})\n",
    "\n",
    "                vali_loss_list.append(vali_loss)\n",
    "                vali_acc_list.append(vali_acc)\n",
    "\n",
    "                file_name = 'regular_training_epoch_%d.ckpt' % (i+1)\n",
    "                save_path = saver.save(sess, \"regular_train/%s/%s\" % (training_id, file_name))\n",
    "                best_vali_acc = max(vali_acc_list)\n",
    "                best_vali_loss = min(vali_loss_list)\n",
    "\n",
    "                print(\"Epoch: %2d, Validation loss: %9.4f, Best loss:%9.4f, Accuracy: %.4f, Best Accuracy: %.4f\" % (i+1, vali_loss, best_vali_loss, vali_acc, best_vali_acc))    \n",
    "\n",
    "                n = n + 1 if vali_acc <= best_vali_acc else 0\n",
    "                if n > early_stopping:\n",
    "                    print('Early Stopping at epoch %d' % i)\n",
    "                    break\n",
    "                    \n",
    "            X_train_folds.append(vali_X)\n",
    "            y_train_folds.append(vali_y)\n",
    "        \n",
    "        file_name = 'final_model'\n",
    "        save_path = saver.save(sess, \"regular_train/%s/%s\" %(training_id, file_name))\n",
    "        print(\"Model saved in path: %s\" % save_path)\n",
    "        \n",
    "        test_acc = sess.run(acc, feed_dict={X: X_test, y: y_test})\n",
    "        print(\"Final test accuracy: %.4f\" % test_acc)\n",
    "        \n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_list = [10, 30, 50, 70, 90, 100, 120, 140, 160]\n",
    "batch_size_list = [10, 50, 100, 500]\n",
    "learning_rate_list = [0.01, 0.02, 0.05, 0.1]\n",
    "activation_function_list = [tf.nn.relu, tf.nn.elu, tf.nn.leaky_relu, tf.nn.tanh]\n",
    "result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[n_neurons=10, learning_rate=0.010000, activation=<function relu at 0x117f706a8>, batch_size=10]\n",
      "FOLD 1\n",
      "Epoch:  1, Validation loss:    2.3657, Best loss:   2.3657, Accuracy: 0.0955, Best Accuracy: 0.0955\n",
      "Epoch:  2, Validation loss:    2.3657, Best loss:   2.3657, Accuracy: 0.0954, Best Accuracy: 0.0955\n",
      "Epoch:  3, Validation loss:    2.3600, Best loss:   2.3600, Accuracy: 0.1011, Best Accuracy: 0.1011\n",
      "Epoch:  4, Validation loss:    2.3600, Best loss:   2.3600, Accuracy: 0.1011, Best Accuracy: 0.1011\n",
      "Epoch:  5, Validation loss:    2.3600, Best loss:   2.3600, Accuracy: 0.1011, Best Accuracy: 0.1011\n",
      "Epoch:  6, Validation loss:    2.3600, Best loss:   2.3600, Accuracy: 0.1011, Best Accuracy: 0.1011\n",
      "Epoch:  7, Validation loss:    2.3600, Best loss:   2.3600, Accuracy: 0.1011, Best Accuracy: 0.1011\n",
      "Epoch:  8, Validation loss:    2.3600, Best loss:   2.3600, Accuracy: 0.1011, Best Accuracy: 0.1011\n",
      "Epoch:  9, Validation loss:    2.3600, Best loss:   2.3600, Accuracy: 0.1011, Best Accuracy: 0.1011\n",
      "Epoch: 10, Validation loss:    2.3600, Best loss:   2.3600, Accuracy: 0.1011, Best Accuracy: 0.1011\n",
      "Epoch: 11, Validation loss:    2.3600, Best loss:   2.3600, Accuracy: 0.1011, Best Accuracy: 0.1011\n",
      "Early Stopping at epoch 10\n",
      "FOLD 2\n",
      "Epoch:  1, Validation loss:    2.3614, Best loss:   2.3614, Accuracy: 0.0997, Best Accuracy: 0.0997\n",
      "Epoch:  2, Validation loss:    2.3614, Best loss:   2.3614, Accuracy: 0.0997, Best Accuracy: 0.0997\n",
      "Epoch:  3, Validation loss:    2.3614, Best loss:   2.3614, Accuracy: 0.0997, Best Accuracy: 0.0997\n",
      "Epoch:  4, Validation loss:    2.3614, Best loss:   2.3614, Accuracy: 0.0997, Best Accuracy: 0.0997\n",
      "Epoch:  5, Validation loss:    2.3614, Best loss:   2.3614, Accuracy: 0.0997, Best Accuracy: 0.0997\n",
      "Epoch:  6, Validation loss:    2.3614, Best loss:   2.3614, Accuracy: 0.0997, Best Accuracy: 0.0997\n",
      "Epoch:  7, Validation loss:    2.3614, Best loss:   2.3614, Accuracy: 0.0997, Best Accuracy: 0.0997\n",
      "Epoch:  8, Validation loss:    2.3614, Best loss:   2.3614, Accuracy: 0.0997, Best Accuracy: 0.0997\n",
      "Epoch:  9, Validation loss:    2.3614, Best loss:   2.3614, Accuracy: 0.0997, Best Accuracy: 0.0997\n",
      "Epoch: 10, Validation loss:    2.3614, Best loss:   2.3614, Accuracy: 0.0997, Best Accuracy: 0.0997\n",
      "Epoch: 11, Validation loss:    2.3614, Best loss:   2.3614, Accuracy: 0.0997, Best Accuracy: 0.0997\n",
      "Early Stopping at epoch 10\n",
      "FOLD 3\n",
      "Epoch:  1, Validation loss:    2.3644, Best loss:   2.3644, Accuracy: 0.0967, Best Accuracy: 0.0967\n",
      "Epoch:  2, Validation loss:    2.3644, Best loss:   2.3644, Accuracy: 0.0967, Best Accuracy: 0.0967\n",
      "Epoch:  3, Validation loss:    2.3644, Best loss:   2.3644, Accuracy: 0.0967, Best Accuracy: 0.0967\n",
      "Epoch:  4, Validation loss:    2.3644, Best loss:   2.3644, Accuracy: 0.0967, Best Accuracy: 0.0967\n",
      "Epoch:  5, Validation loss:    2.3644, Best loss:   2.3644, Accuracy: 0.0967, Best Accuracy: 0.0967\n",
      "Epoch:  6, Validation loss:    2.3644, Best loss:   2.3644, Accuracy: 0.0967, Best Accuracy: 0.0967\n",
      "Epoch:  7, Validation loss:    2.3644, Best loss:   2.3644, Accuracy: 0.0967, Best Accuracy: 0.0967\n",
      "Epoch:  8, Validation loss:    2.3644, Best loss:   2.3644, Accuracy: 0.0967, Best Accuracy: 0.0967\n",
      "Epoch:  9, Validation loss:    2.3644, Best loss:   2.3644, Accuracy: 0.0967, Best Accuracy: 0.0967\n",
      "Epoch: 10, Validation loss:    2.3644, Best loss:   2.3644, Accuracy: 0.0967, Best Accuracy: 0.0967\n",
      "Epoch: 11, Validation loss:    2.3644, Best loss:   2.3644, Accuracy: 0.0967, Best Accuracy: 0.0967\n",
      "Early Stopping at epoch 10\n",
      "Model saved in path: regular_train/[n_neurons_10, learning_rate_0.010000, activation_<function relu at 0x117f706a8>, batch_size_10]-20180418131418/final_model\n",
      "Final test accuracy: 0.0992\n",
      "[n_neurons=10, learning_rate=0.020000, activation=<function relu at 0x117f706a8>, batch_size=10]\n",
      "FOLD 1\n",
      "Epoch:  1, Validation loss:    2.3591, Best loss:   2.3591, Accuracy: 0.1020, Best Accuracy: 0.1020\n",
      "Epoch:  2, Validation loss:    2.3591, Best loss:   2.3591, Accuracy: 0.1020, Best Accuracy: 0.1020\n",
      "Epoch:  3, Validation loss:    2.3591, Best loss:   2.3591, Accuracy: 0.1020, Best Accuracy: 0.1020\n",
      "Epoch:  4, Validation loss:    2.3591, Best loss:   2.3591, Accuracy: 0.1020, Best Accuracy: 0.1020\n",
      "Epoch:  5, Validation loss:    2.3591, Best loss:   2.3591, Accuracy: 0.1020, Best Accuracy: 0.1020\n",
      "Epoch:  6, Validation loss:    2.3591, Best loss:   2.3591, Accuracy: 0.1020, Best Accuracy: 0.1020\n",
      "Epoch:  7, Validation loss:    2.3591, Best loss:   2.3591, Accuracy: 0.1020, Best Accuracy: 0.1020\n",
      "Epoch:  8, Validation loss:    2.3591, Best loss:   2.3591, Accuracy: 0.1020, Best Accuracy: 0.1020\n",
      "Epoch:  9, Validation loss:    2.3591, Best loss:   2.3591, Accuracy: 0.1020, Best Accuracy: 0.1020\n",
      "Epoch: 10, Validation loss:    2.3591, Best loss:   2.3591, Accuracy: 0.1020, Best Accuracy: 0.1020\n",
      "Epoch: 11, Validation loss:    2.3591, Best loss:   2.3591, Accuracy: 0.1020, Best Accuracy: 0.1020\n",
      "Early Stopping at epoch 10\n",
      "FOLD 2\n",
      "Epoch:  1, Validation loss:    2.3585, Best loss:   2.3585, Accuracy: 0.1027, Best Accuracy: 0.1027\n",
      "Epoch:  2, Validation loss:    2.3585, Best loss:   2.3585, Accuracy: 0.1027, Best Accuracy: 0.1027\n",
      "Epoch:  3, Validation loss:    2.3585, Best loss:   2.3585, Accuracy: 0.1027, Best Accuracy: 0.1027\n",
      "Epoch:  4, Validation loss:    2.3585, Best loss:   2.3585, Accuracy: 0.1027, Best Accuracy: 0.1027\n",
      "Epoch:  5, Validation loss:    2.3585, Best loss:   2.3585, Accuracy: 0.1027, Best Accuracy: 0.1027\n",
      "Epoch:  6, Validation loss:    2.3585, Best loss:   2.3585, Accuracy: 0.1027, Best Accuracy: 0.1027\n",
      "Epoch:  7, Validation loss:    2.3585, Best loss:   2.3585, Accuracy: 0.1027, Best Accuracy: 0.1027\n",
      "Epoch:  8, Validation loss:    2.3585, Best loss:   2.3585, Accuracy: 0.1027, Best Accuracy: 0.1027\n",
      "Epoch:  9, Validation loss:    2.3585, Best loss:   2.3585, Accuracy: 0.1027, Best Accuracy: 0.1027\n",
      "Epoch: 10, Validation loss:    2.3585, Best loss:   2.3585, Accuracy: 0.1027, Best Accuracy: 0.1027\n",
      "Epoch: 11, Validation loss:    2.3585, Best loss:   2.3585, Accuracy: 0.1027, Best Accuracy: 0.1027\n",
      "Early Stopping at epoch 10\n",
      "FOLD 3\n",
      "Epoch:  1, Validation loss:    2.3540, Best loss:   2.3540, Accuracy: 0.1071, Best Accuracy: 0.1071\n",
      "Epoch:  2, Validation loss:    2.3540, Best loss:   2.3540, Accuracy: 0.1071, Best Accuracy: 0.1071\n",
      "Epoch:  3, Validation loss:    2.3540, Best loss:   2.3540, Accuracy: 0.1071, Best Accuracy: 0.1071\n",
      "Epoch:  4, Validation loss:    2.3540, Best loss:   2.3540, Accuracy: 0.1071, Best Accuracy: 0.1071\n",
      "Epoch:  5, Validation loss:    2.3540, Best loss:   2.3540, Accuracy: 0.1071, Best Accuracy: 0.1071\n",
      "Epoch:  6, Validation loss:    2.3540, Best loss:   2.3540, Accuracy: 0.1071, Best Accuracy: 0.1071\n",
      "Epoch:  7, Validation loss:    2.3540, Best loss:   2.3540, Accuracy: 0.1071, Best Accuracy: 0.1071\n",
      "Epoch:  8, Validation loss:    2.3540, Best loss:   2.3540, Accuracy: 0.1071, Best Accuracy: 0.1071\n",
      "Epoch:  9, Validation loss:    2.3540, Best loss:   2.3540, Accuracy: 0.1071, Best Accuracy: 0.1071\n",
      "Epoch: 10, Validation loss:    2.3540, Best loss:   2.3540, Accuracy: 0.1071, Best Accuracy: 0.1071\n",
      "Epoch: 11, Validation loss:    2.3540, Best loss:   2.3540, Accuracy: 0.1071, Best Accuracy: 0.1071\n",
      "Early Stopping at epoch 10\n",
      "Model saved in path: regular_train/[n_neurons_10, learning_rate_0.020000, activation_<function relu at 0x117f706a8>, batch_size_10]-20180418131846/final_model\n",
      "Final test accuracy: 0.1039\n",
      "[n_neurons=10, learning_rate=0.050000, activation=<function relu at 0x117f706a8>, batch_size=10]\n",
      "FOLD 1\n",
      "Epoch:  1, Validation loss:    2.2657, Best loss:   2.2657, Accuracy: 0.1523, Best Accuracy: 0.1523\n",
      "Epoch:  2, Validation loss:    2.2662, Best loss:   2.2657, Accuracy: 0.1443, Best Accuracy: 0.1523\n",
      "Epoch:  3, Validation loss:    2.3022, Best loss:   2.2657, Accuracy: 0.1020, Best Accuracy: 0.1523\n",
      "Epoch:  4, Validation loss:    2.3020, Best loss:   2.2657, Accuracy: 0.1020, Best Accuracy: 0.1523\n",
      "Epoch:  5, Validation loss:    2.3020, Best loss:   2.2657, Accuracy: 0.1020, Best Accuracy: 0.1523\n",
      "Epoch:  6, Validation loss:    2.3020, Best loss:   2.2657, Accuracy: 0.1020, Best Accuracy: 0.1523\n",
      "Epoch:  7, Validation loss:    2.3020, Best loss:   2.2657, Accuracy: 0.1020, Best Accuracy: 0.1523\n",
      "Epoch:  8, Validation loss:    2.3020, Best loss:   2.2657, Accuracy: 0.1020, Best Accuracy: 0.1523\n",
      "Epoch:  9, Validation loss:    2.3020, Best loss:   2.2657, Accuracy: 0.1020, Best Accuracy: 0.1523\n",
      "Epoch: 10, Validation loss:    2.3020, Best loss:   2.2657, Accuracy: 0.1020, Best Accuracy: 0.1523\n",
      "Epoch: 11, Validation loss:    2.3020, Best loss:   2.2657, Accuracy: 0.1020, Best Accuracy: 0.1523\n",
      "Early Stopping at epoch 10\n",
      "FOLD 2\n",
      "Epoch:  1, Validation loss:    2.3022, Best loss:   2.3022, Accuracy: 0.1144, Best Accuracy: 0.1144\n",
      "Epoch:  2, Validation loss:    2.3022, Best loss:   2.3022, Accuracy: 0.1144, Best Accuracy: 0.1144\n",
      "Epoch:  3, Validation loss:    2.3022, Best loss:   2.3022, Accuracy: 0.1144, Best Accuracy: 0.1144\n",
      "Epoch:  4, Validation loss:    2.3022, Best loss:   2.3022, Accuracy: 0.1144, Best Accuracy: 0.1144\n",
      "Epoch:  5, Validation loss:    2.3022, Best loss:   2.3022, Accuracy: 0.1144, Best Accuracy: 0.1144\n",
      "Epoch:  6, Validation loss:    2.3022, Best loss:   2.3022, Accuracy: 0.1144, Best Accuracy: 0.1144\n",
      "Epoch:  7, Validation loss:    2.3022, Best loss:   2.3022, Accuracy: 0.1144, Best Accuracy: 0.1144\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-739336fa7491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_rate_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0maf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactivation_function_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                 \u001b[0mtraining_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[n_neurons=%d, learning_rate=%f, activation=%s, batch_size=%d]'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-d1e55f1cf031>\u001b[0m in \u001b[0;36mDNN\u001b[0;34m(epoch, n_neurons, learning_rate, activation, batch_size, early_stopping, restore_model_name)\u001b[0m\n\u001b[1;32m     44\u001b[0m                         \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                         \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n in neurons_list:\n",
    "    for bs in batch_size_list:\n",
    "        for lr in learning_rate_list:\n",
    "            for af in activation_function_list:\n",
    "                test_acc = DNN(epoch=1000, n_neurons=n, learning_rate=lr, activation=af, batch_size=bs, early_stopping=10)\n",
    "                training_id = '[n_neurons=%d, learning_rate=%f, activation=%s, batch_size=%d]' % (n, lr, af, bs)\n",
    "                result[training_id] = test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[n_neurons=10, learning_rate=0.010000, activation=<function relu at 0x117f706a8>, batch_size=10]': 0.09916364,\n",
       " '[n_neurons=10, learning_rate=0.020000, activation=<function relu at 0x117f706a8>, batch_size=10]': 0.10390909}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
