{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taiwan_201701 = pd.read_csv('sinica/201701_Taiwan.csv')\n",
    "# Taiwan_201702 = pd.read_csv('sinica/201702_Taiwan.csv')\n",
    "# Taiwan_201703 = pd.read_csv('sinica/201703_Taiwan.csv')\n",
    "\n",
    "# Taiwan_201701 = Taiwan_201701.rename(columns={' lat': 'lat', ' lon': 'lon'})\n",
    "# Taiwan_201702 = Taiwan_201702.rename(columns={' lat': 'lat', ' lon': 'lon'})\n",
    "\n",
    "epa_loc_data_201701 = pd.read_csv('epa/EPA_LOC_data_201701.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Taiwan_201701_loc = pd.read_csv('Taiwan_201701_loc.csv')\n",
    "Taiwan_201702_loc = pd.read_csv('Taiwan_201702_loc.csv')\n",
    "Taiwan_201703_loc = pd.read_csv('Taiwan_201703_loc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Taiwan_201701_loc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers(df):\n",
    "    \n",
    "    def _drop_PM10(df):\n",
    "        PM10_CI95p = df.PM10.mean() + 2 * df.PM10.std()\n",
    "        PM10_CI95n = df.PM10.mean() - 2 * df.PM10.std()\n",
    "        df = df.drop(df[df.PM10 > PM10_CI95p].index)\n",
    "        df = df.drop(df[df.PM10 < PM10_CI95n].index)\n",
    "        df = df.drop(df[df.PM10 == 0].index)\n",
    "        return df\n",
    "\n",
    "    def _drop_PM1(df):\n",
    "        PM1_CI95p = df.PM1.mean() + 2 * df.PM1.std()\n",
    "        PM1_CI95n = df.PM1.mean() - 2 * df.PM1.std()\n",
    "        df = df.drop(df[df.PM1 > PM1_CI95p].index)\n",
    "        df = df.drop(df[df.PM1 < PM1_CI95n].index)\n",
    "        df = df.drop(df[df.PM1 == 0].index)\n",
    "        return df\n",
    "    \n",
    "    def _drop_temperature(df):\n",
    "        temperature_CI95p = df.Temperature.mean() + 2 * df.Temperature.std()\n",
    "        temperature_CI95n = df.Temperature.mean() - 2 * df.Temperature.std()\n",
    "        df = df.drop(df[df.Temperature > temperature_CI95p].index)\n",
    "        df = df.drop(df[df.Temperature < temperature_CI95n].index)\n",
    "        return df\n",
    "    \n",
    "    def _drop_humidity(df):\n",
    "        humidity_CI95p = df.Humidity.mean() + 2 * df.Humidity.std()\n",
    "        humidity_CI95n = df.Humidity.mean() - 2 * df.Humidity.std()\n",
    "        df = df.drop(df[df.Humidity > humidity_CI95p].index)\n",
    "        df = df.drop(df[df.Humidity < humidity_CI95n].index)\n",
    "        return df\n",
    "    \n",
    "    before = df.shape\n",
    "    \n",
    "    df = _drop_PM10(df)\n",
    "    df = _drop_PM1(df)\n",
    "    df = _drop_temperature(df)\n",
    "    df = _drop_humidity(df)\n",
    "    \n",
    "    after = df.shape\n",
    "    \n",
    "    print(before, ' -> ', after)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taiwan_201701_CI95 = drop_outliers(Taiwan_201701_loc)\n",
    "# Taiwan_201702_CI95 = drop_outliers(Taiwan_201702_loc)\n",
    "# Taiwan_201703_CI95 = drop_outliers(Taiwan_201703_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    \n",
    "    def _normalization(df):\n",
    "        for feature_name in ['PM10', 'PM1', 'Temperature', 'Humidity']:\n",
    "            max_value = df[feature_name].mean() + 2 * df[feature_name].std()\n",
    "            min_value = df[feature_name].mean() - 2 * df[feature_name].std()\n",
    "            df[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "        print('normalization DONE!')\n",
    "        return df\n",
    "    \n",
    "    def _concat_datetime(df):\n",
    "        df['period'] = df[['Date', 'Time']].apply(lambda x: ' '.join(x), axis=1)\n",
    "        df = df.drop(['Date', 'Time'], axis=1)\n",
    "        print('concat_datetime DONE!')\n",
    "        return df\n",
    "    \n",
    "    def _cluster_loc(df):\n",
    "        global epa_loc_data_201701\n",
    "        counter = 0\n",
    "        loc_list = []\n",
    "        display_steps = int(len(df) / 100)\n",
    "        for lon, lat in zip(df['lon'], df['lat']):\n",
    "            min_distance = 999.9\n",
    "            for row in epa_loc_data_201701.itertuples():\n",
    "                loc_name = row[1]\n",
    "                loc_lon = row[2]\n",
    "                loc_lat = row[3]\n",
    "                distance = (lon-loc_lon) ** 2 + (lat-lat) ** 2\n",
    "                if min_distance > distance:\n",
    "                    min_distance = distance\n",
    "                    best_loc = loc_name\n",
    "            loc_list.append(best_loc)\n",
    "            if counter % display_steps == 0:\n",
    "                p = int(counter / display_steps)\n",
    "                print('[%s] %d/100' % (time.strftime(\"%H:%M:%S\", time.localtime()), p))\n",
    "            counter += 1\n",
    "        df['loc'] = pd.Series(loc_list).values\n",
    "        print('cluster_loc DONE!')\n",
    "        return df\n",
    "    \n",
    "    def _drop_redundant_features(df):\n",
    "        df = df.drop(['device_id', 'lat', 'lon'], axis=1)\n",
    "        print('drop_redundant_features DONE!')\n",
    "        return df\n",
    "    \n",
    "    df = _normalization(df)\n",
    "    df = _concat_datetime(df)\n",
    "#     df = _cluster_loc(df)\n",
    "    df = _drop_redundant_features(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Taiwan_201701_precd = preprocessing(Taiwan_201701_loc)\n",
    "Taiwan_201702_precd = preprocessing(Taiwan_201702_loc)\n",
    "Taiwan_201703_precd = preprocessing(Taiwan_201703_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Taiwan_201701_group = Taiwan_201701_precd.groupby('loc') \n",
    "Taiwan_201702_group = Taiwan_201702_precd.groupby('loc')\n",
    "Taiwan_201703_group = Taiwan_201703_precd.groupby('loc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_group(df):\n",
    "    max_name = ''\n",
    "    max_len = 0\n",
    "    for name, group in df:\n",
    "        length = len(group)\n",
    "        if max_len < length:\n",
    "            max_name = name\n",
    "            max_len = length\n",
    "    print(max_name, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_group = Taiwan_201701_group.get_group('臺中市南屯區')\n",
    "valid_group = Taiwan_201702_group.get_group('臺南市中西區')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing of groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_datetime_group(df):\n",
    "    \n",
    "    def _convert_datetime_to_norm_sec(df):\n",
    "        df['period'] = pd.to_datetime(df['period'])\n",
    "        df['seconds'] = [time.mktime(t.timetuple()) for t in df.period]\n",
    "        max_value = df['seconds'].max()\n",
    "        min_value = df['seconds'].min()\n",
    "        df['seconds'] = (df['seconds'] - min_value) / (max_value - min_value)\n",
    "        return df\n",
    "    \n",
    "    def _drop_redundant_features(df):\n",
    "        X = df.drop(['loc', 'period', 'PM2.5'], axis=1).values\n",
    "        Y = df['PM2.5'].values\n",
    "        return X, Y\n",
    "    \n",
    "    def _reshapeX(X):\n",
    "        X = X.reshape(X.shape[0], 1, X.shape[-1])\n",
    "        return X\n",
    "        \n",
    "#     df = _convert_datetime_to_norm_sec(df)\n",
    "    X, Y = _drop_redundant_features(df)\n",
    "    X = _reshapeX(X)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = preprocessing_datetime_group(train_group)\n",
    "validX, validY = preprocessing_datetime_group(valid_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainX.shape, trainY.shape, validX.shape, validY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def LSTM_PM25(trainX, trainY, validX, validY, output_dim, epoch, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(output_dim, input_shape=(trainX.shape[1], trainX.shape[2])))    \n",
    "    model.add(Dense(units=1, kernel_initializer='uniform', activation='relu'))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    history = model.fit(trainX, trainY, epochs=epoch, batch_size=batch_size, validation_data=(validX, validY), verbose=1, shuffle=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LSTM_PM25(trainX, trainY, validX, validY, 50, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model15 = LSTM_PM25(trainX, trainY, validX, validY, 50, 15, 100)\n",
    "model20 = LSTM_PM25(trainX, trainY, validX, validY, 50, 20, 100)\n",
    "model30 = LSTM_PM25(trainX, trainY, validX, validY, 50, 30, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model40 = LSTM_PM25(trainX, trainY, validX, validY, 50, 40, 100)\n",
    "model50 = LSTM_PM25(trainX, trainY, validX, validY, 50, 50, 100)\n",
    "model60 = LSTM_PM25(trainX, trainY, validX, validY, 50, 60, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Predict 201703 PM2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_201703_PM25(model):\n",
    "    result = [None] * len(Taiwan_201703_precd)\n",
    "\n",
    "    counter = 0\n",
    "    total = len(Taiwan_201703_group)\n",
    "\n",
    "    for name, group in Taiwan_201703_group:\n",
    "        index_list = group.index.tolist()\n",
    "        testX = group.drop(['PM2.5', 'loc', 'period'], axis=1)\n",
    "        testX = testX.values\n",
    "        testX = testX.reshape(testX.shape[0], 1, testX.shape[-1])\n",
    "        testY_hat_list = model.predict(testX).tolist()\n",
    "\n",
    "        for index, Y_hat in zip(index_list, testY_hat_list):\n",
    "            result[index] = Y_hat[0]\n",
    "\n",
    "        counter += 1\n",
    "        print('%s Finished! %d/%d' % (name, counter, total))\n",
    "        \n",
    "        result_df = pd.DataFrame(result, columns=[\"PM2.5\"])\n",
    "        result_df = result_df.round(0)\n",
    "        \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result15 = predict_201703_PM25(model15)\n",
    "result20 = predict_201703_PM25(model20)\n",
    "result30 = predict_201703_PM25(model30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result40 = predict_201703_PM25(model40)\n",
    "result50 = predict_201703_PM25(model50)\n",
    "result60 = predict_201703_PM25(model60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result15.to_csv('output_epoch15.csv', index=False, header=True)\n",
    "result20.to_csv('output_epoch20.csv', index=False, header=True)\n",
    "result30.to_csv('output_epoch30.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result40.to_csv('output_epoch40.csv', index=False, header=True)\n",
    "result50.to_csv('output_epoch50.csv', index=False, header=True)\n",
    "result60.to_csv('output_epoch60.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
